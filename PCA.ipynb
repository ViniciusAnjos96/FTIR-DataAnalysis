{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlh/nzw1OpC3cd9NYagXXb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeETWyZovh_H"
      },
      "outputs": [],
      "source": [
        "# PCA\n",
        "# Change the NAME.csv\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('NAME.csv')\n",
        "\n",
        "# Assuming the first column is the class and the rest are the spectral data\n",
        "# Extract the features (spectral data) and the target (class)\n",
        "X = df.iloc[:, 1:]\n",
        "y = df.iloc[:, 0]\n",
        "\n",
        "# Perform PCA\n",
        "# We'll reduce the dimensionality to 2 components for visualization\n",
        "n_components = 2\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Create a DataFrame with the PCA results\n",
        "pca_df = pd.DataFrame(data=X_pca, columns=[f'Principal Component {i+1}' for i in range(n_components)])\n",
        "pca_df['class'] = y\n",
        "\n",
        "# Visualize the PCA results\n",
        "plt.figure(figsize=(8, 6))\n",
        "classes = y.unique()\n",
        "colors = ['r', 'g', 'b'] # You can add more colors if you have more classes\n",
        "\n",
        "for i, class_name in enumerate(classes):\n",
        "    plt.scatter(pca_df.loc[pca_df['class'] == class_name, 'Principal Component 1'],\n",
        "                pca_df.loc[pca_df['class'] == class_name, 'Principal Component 2'],\n",
        "                c=colors[i],\n",
        "                label=class_name)\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA of FTIR Spectra')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: Display the explained variance ratio\n",
        "print(f'Explained variance ratio by component: {pca.explained_variance_ratio_}')\n",
        "print(f'Total explained variance: {sum(pca.explained_variance_ratio_)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a simple classifier (e.g., Random Forest) for demonstration\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
        "\n",
        "# Display the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8rhlHxnE8Gd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifiaction Report\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Prepare data for the table\n",
        "metrics = ['precision', 'recall', 'f1-score', 'support']\n",
        "classes = list(report.keys())[:-3]  # Get class names, excluding accuracy, macro avg, weighted avg\n",
        "\n",
        "data = []\n",
        "for cls in classes:\n",
        "    row = [cls]\n",
        "    for metric in metrics:\n",
        "        row.append(f\"{report[cls][metric]:.4f}\")\n",
        "    data.append(row)\n",
        "\n",
        "# Add accuracy, macro avg, weighted avg\n",
        "accuracy_row = ['accuracy', '', '', f\"{report['accuracy']:.4f}\", f\"{report['accuracy']:.0f}\"] # Support for accuracy is the total number of samples\n",
        "macro_avg_row = ['macro avg']\n",
        "for metric in metrics[:-1]: # Exclude support for macro avg\n",
        "    macro_avg_row.append(f\"{report['macro avg'][metric]:.4f}\")\n",
        "macro_avg_row.append(f\"{report['macro avg']['support']:.0f}\")\n",
        "\n",
        "weighted_avg_row = ['weighted avg']\n",
        "for metric in metrics[:-1]: # Exclude support for weighted avg\n",
        "    weighted_avg_row.append(f\"{report['weighted avg'][metric]:.4f}\")\n",
        "weighted_avg_row.append(f\"{report['weighted avg']['support']:.0f}\")\n",
        "\n",
        "\n",
        "data.append(macro_avg_row)\n",
        "data.append(weighted_avg_row)\n",
        "data.append(accuracy_row)\n",
        "\n",
        "\n",
        "# Create a DataFrame for the table\n",
        "df_report = pd.DataFrame(data, columns=['class'] + metrics)\n",
        "\n",
        "# Create the figure and axis\n",
        "fig, ax = plt.subplots(figsize=(8, 4))  # Adjust size as needed\n",
        "\n",
        "# Hide the axes\n",
        "ax.axis('off')\n",
        "ax.axis('tight')\n",
        "\n",
        "# Create the table\n",
        "table = ax.table(cellText=df_report.values,\n",
        "                 colLabels=df_report.columns,\n",
        "                 cellLoc = 'center',\n",
        "                 loc='center')\n",
        "\n",
        "# Style the table (optional)\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1.2, 1.2) # Adjust scale as needed\n",
        "\n",
        "# Set title\n",
        "ax.set_title('Classification Report', fontsize=14)\n",
        "\n",
        "# Display the table\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "b79q1MWN-Ayp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# contribution of each variable (Cos2).\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Move the import statement to the top\n",
        "\n",
        "# Calculate the squared cosines (Cos2) for each variable\n",
        "# Cos2 of a variable i for a principal component j is (loading_ij)^2\n",
        "# Loadings are the eigenvectors scaled by the square root of the eigenvalues\n",
        "\n",
        "# The loadings are the components attribute of the PCA object\n",
        "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
        "\n",
        "# Calculate squared cosines\n",
        "cos2 = loadings**2\n",
        "\n",
        "# Sum of squared cosines for each variable across all principal components\n",
        "total_cos2_per_variable = np.sum(cos2, axis=1)\n",
        "\n",
        "# Normalize the Cos2 values to show the contribution to the explained variance\n",
        "# This can be interpreted as the proportion of variance of each variable that is captured by the selected principal components.\n",
        "# Alternatively, you might want to show the contribution of each variable to the total inertia explained by the selected components.\n",
        "# A common approach is to look at the contribution of each variable to each principal component: (cos2_ij / eigenvalue_j) * 100\n",
        "# Let's plot the total Cos2 for each variable as a measure of its representation in the principal components.\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(X.shape[1]), total_cos2_per_variable)\n",
        "plt.xlabel('Variable Index')\n",
        "plt.ylabel('Total Cos2 (Contribution to PCs)')\n",
        "plt.title('Contribution of Each Variable to the Principal Components')\n",
        "plt.xticks(range(X.shape[1]), rotation=90)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n",
        "\n",
        "# You can also show the contribution of each variable to the first two principal components separately\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(X.shape[1]), cos2[:, 0], label='PC 1')\n",
        "plt.bar(range(X.shape[1]), cos2[:, 1], bottom=cos2[:, 0], label='PC 2')\n",
        "plt.xlabel('Variable Index')\n",
        "plt.ylabel('Cos2 (Contribution)')\n",
        "plt.title('Contribution of Each Variable to PC1 and PC2')\n",
        "plt.xticks(range(X.shape[1]), rotation=90)\n",
        "plt.legend()\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "I8lKYn9Cx22J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SHapley Additive exPlanations\n",
        "\n",
        "!pip install shap\n",
        "\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Assuming you have a trained model. Let's train a simple classifier as an example.\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Create an explainer object.\n",
        "# For tree models, shap.TreeExplainer is recommended.\n",
        "explainer = shap.TreeExplainer(model)\n",
        "\n",
        "# Calculate SHAP values for the test set.\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Assuming a multi-class classification, shap_values will be a list of arrays,\n",
        "# one for each class. Let's pick the SHAP values for the first class as an example.\n",
        "# You might need to choose the class that is of most interest for your analysis.\n",
        "# shap_values_class_0 = shap_values[0]\n",
        "\n",
        "# Summarize the effects of all the features.\n",
        "# This plot shows the importance of each feature.\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "# Another way to summarize is using a bar plot.\n",
        "# This plot shows the average absolute SHAP value for each feature.\n",
        "# If you have multi-class output, you might need to specify which class you are interested in,\n",
        "# or use a different visualization type suitable for multi-class.\n",
        "# shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "\n",
        "# Visualize the SHAP values for a single prediction.\n",
        "# Choose an instance from the test set (e.g., the first instance).\n",
        "# instance_index = 0\n",
        "# shap.initjs() # Initialize JavaScript for interactive plots\n",
        "# shap.force_plot(explainer.expected_value[0], shap_values_class_0[instance_index,:], X_test.iloc[instance_index,:])\n",
        "\n",
        "# To visualize contributions for a specific class prediction (e.g., class 0):\n",
        "# shap.force_plot(explainer.expected_value[0], shap_values[0][instance_index,:], X_test.iloc[instance_index,:])\n",
        "\n",
        "# For multi-class, you can visualize the force plot for a specific class or a combination.\n",
        "# Here's an example for the first instance, visualizing contributions to the prediction of class 0\n",
        "# shap.initjs()\n",
        "# shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:])\n",
        "\n",
        "# You can also visualize the dependence of a feature's effect on its value.\n",
        "# Choose a feature to plot (e.g., the first feature).\n",
        "# feature_index_to_plot = 0\n",
        "# shap.dependence_plot(feature_index_to_plot, shap_values[0], X_test)\n",
        "\n",
        "# If you have multi-class, you might need to specify the class\n",
        "# shap.dependence_plot(feature_index_to_plot, shap_values[0], X_test, interaction_index=None) # for class 0\n",
        "\n",
        "# You might want to explore different SHAP plots based on your specific model and analysis goals.\n",
        "# The interpretation of SHAP values can vary slightly depending on the model type and the explainer used.\n",
        "# Refer to the SHAP documentation for more details on different plot types and interpretations:\n",
        "# https://shap.readthedocs.io/en/latest/api.html#plots\n"
      ],
      "metadata": {
        "id": "_7qWTqAu3M8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Local Interpretable Model-agnostic Explanations\n",
        "\n",
        "!pip install lime\n",
        "\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "# Assuming you have a trained model.\n",
        "# We already have a RandomForestClassifier trained as `model`.\n",
        "\n",
        "# Create a LIME explainer\n",
        "# We need to provide the training data, the feature names, and the class names.\n",
        "# The mode is 'classification' for classification problems.\n",
        "# The discretize_continuous flag is important for LIME.\n",
        "explainer_lime = lime.lime_tabular.LimeTabularExplainer(training_data=X_train.values,\n",
        "                                                        feature_names=X_train.columns.tolist(),\n",
        "                                                        class_names=model.classes_.tolist(),\n",
        "                                                        mode='classification',\n",
        "                                                        discretize_continuous=True)\n",
        "\n",
        "# Explain a single instance\n",
        "# Choose an instance from the test set to explain (e.g., the first instance).\n",
        "instance_to_explain_index = 0\n",
        "instance_to_explain = X_test.iloc[instance_to_explain_index]\n",
        "\n",
        "# Get the explanation for the instance.\n",
        "# num_features controls how many features are included in the explanation.\n",
        "explanation = explainer_lime.explain_instance(data_row=instance_to_explain.values,\n",
        "                                              predict_fn=model.predict_proba,\n",
        "                                              num_features=5)\n",
        "\n",
        "# Print the explanation.\n",
        "print(f\"Explanation for instance {instance_to_explain_index}:\")\n",
        "print(explanation.as_list())\n",
        "\n",
        "# Visualize the explanation\n",
        "# You can also save the explanation as an HTML file.\n",
        "explanation.show_in_notebook(show_table=True, show_all=False)\n",
        "\n",
        "# You can get the list of (feature, weight) tuples\n",
        "# lime_features = explanation.as_list()\n",
        "\n",
        "# For a specific prediction (e.g., the predicted class):\n",
        "predicted_class_index = model.predict(instance_to_explain.values.reshape(1, -1))[0]\n",
        "# If your target is not integer encoded, you might need to map the index back to the class name\n",
        "# predicted_class_name = model.classes_[predicted_class_index]\n",
        "\n",
        "# To get explanation for a specific class prediction:\n",
        "# explanation_for_class = explainer_lime.explain_instance(data_row=instance_to_explain.values,\n",
        "#                                                          predict_fn=model.predict_proba,\n",
        "#                                                          num_features=5,\n",
        "#                                                          top_labels=None, # Set to 1 to explain the top predicted class only\n",
        "#                                                          labels=(predicted_class_index,)) # Specify the label you want to explain\n",
        "\n",
        "# explanation_for_class.show_in_notebook(show_table=True, show_all=False)\n",
        "\n",
        "# LIME provides local explanations for individual predictions.\n",
        "# The interpretation relies on understanding how perturbing the feature values around the instance\n",
        "# affects the model's prediction. The explanation highlights which features are most influential\n",
        "# in predicting the specific outcome for that specific instance."
      ],
      "metadata": {
        "id": "TxAfD08f3ZL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Other PCs combination.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# Assuming X has been defined and PCA has been performed\n",
        "# Update n_components to include the desired number of principal components\n",
        "n_components = 4  # For PC1, PC2, PC3, PC4\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Create a DataFrame with the PCA results\n",
        "pca_df = pd.DataFrame(data=X_pca, columns=[f'PC{i+1}' for i in range(n_components)])\n",
        "pca_df['class'] = y\n",
        "\n",
        "# Get unique classes and assign colors\n",
        "classes = y.unique()\n",
        "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k'][:len(classes)] # Ensure enough colors\n",
        "\n",
        "# Create a figure with subplots for all combinations of PCs\n",
        "fig, axes = plt.subplots(n_components, n_components, figsize=(20, 20)) # Adjust figsize as needed\n",
        "\n",
        "for i in range(n_components):\n",
        "    for j in range(n_components):\n",
        "        ax = axes[i, j]\n",
        "\n",
        "        # Plot the scatter plot for each class\n",
        "        for k, class_name in enumerate(classes):\n",
        "            ax.scatter(pca_df.loc[pca_df['class'] == class_name, f'PC{j+1}'],\n",
        "                       pca_df.loc[pca_df['class'] == class_name, f'PC{i+1}'],\n",
        "                       c=colors[k],\n",
        "                       label=class_name,\n",
        "                       alpha=0.6, s=50) # Add some transparency and size for better visualization\n",
        "\n",
        "        # Set labels and title for each subplot\n",
        "        ax.set_xlabel(f'PC{j+1}')\n",
        "        ax.set_ylabel(f'PC{i+1}')\n",
        "        ax.set_title(f'PC{j+1} vs PC{i+1}')\n",
        "\n",
        "        # Add legend to one of the subplots (to avoid repetition)\n",
        "        if i == 0 and j == n_components - 1:\n",
        "             ax.legend(title='Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Adjust layout to prevent overlapping titles and labels\n",
        "plt.tight_layout(rect=[0, 0, 0.9, 1]) # Adjust rect to make space for the legend\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Optional: Display the explained variance ratio\n",
        "print(f'Explained variance ratio by component: {pca.explained_variance_ratio_}')\n",
        "print(f'Total explained variance: {sum(pca.explained_variance_ratio_)}')\n",
        "\n"
      ],
      "metadata": {
        "id": "WM-nwWUz2ivg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}