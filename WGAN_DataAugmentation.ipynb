{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUoYxgX2koA9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import grad\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebUjH5pWlH90"
      },
      "outputs": [],
      "source": [
        "# === Ler os dados .CSV / Read .CSV data ===\n",
        "dados = pd.read_csv('NAME.csv', sep=',')  # especifica o separador / specifies the separator\n",
        "print(dados.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWeW9pNFlI5D"
      },
      "outputs": [],
      "source": [
        "# ===== 1. Ler os dados .XLSX / Read .XLSX data =====\n",
        "dados = pd.read_excel(\"NAME.xlsx\")\n",
        "print(dados.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QADBF0ZHkqL7"
      },
      "outputs": [],
      "source": [
        "# ===== Exclui NaN / Excludes NaN =====\n",
        "dados_cleaned = dados.dropna()\n",
        "\n",
        "classes = dados.iloc[1:, 0].values   # primeira coluna (classes) / first column (classes)\n",
        "features = dados.columns[1:]         # primeira linha (nomes das features) / first line (feature names)\n",
        "X = dados.iloc[1:, 1:].astype(float).values\n",
        "\n",
        "# Normalizar para [0,1] / Normalize to [0,1]\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkWsL6nXkmeo"
      },
      "outputs": [],
      "source": [
        "# ========= 2. Definir modelos WGAN / Define WGAN models =========\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_dim),\n",
        "            nn.Sigmoid()  # espectros normalizados [0,1] / normalized spectra [0,1]\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# ========= 3. Função Gradient Penalty / Gradient Penalty Function =========\n",
        "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
        "    batch_size, features = real.shape\n",
        "    epsilon = torch.rand((batch_size, 1)).to(device)\n",
        "    interpolated = real * epsilon + fake * (1 - epsilon)\n",
        "    interpolated.requires_grad_(True)\n",
        "    mixed_scores = critic(interpolated)\n",
        "    gradient = grad(\n",
        "        outputs=mixed_scores,\n",
        "        inputs=interpolated,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gp = ((gradient.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gp\n",
        "\n",
        "# ========= 4. Função de treino por classe / Training function by class =========\n",
        "def treinar_wgan(X_class, n_epochs=500, batch_size=32, noise_dim=50, n_fake=50, device=\"cpu\"):\n",
        "    X_tensor = torch.tensor(X_class, dtype=torch.float32).to(device)\n",
        "\n",
        "    gen = Generator(noise_dim, X_class.shape[1]).to(device)\n",
        "    critic = Critic(X_class.shape[1]).to(device)\n",
        "\n",
        "    opt_gen = optim.Adam(gen.parameters(), lr=1e-4, betas=(0.0, 0.9))\n",
        "    opt_critic = optim.Adam(critic.parameters(), lr=1e-4, betas=(0.0, 0.9))\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for i in range(0, len(X_tensor), batch_size):\n",
        "            real = X_tensor[i:i+batch_size]\n",
        "            cur_batch_size = real.shape[0]\n",
        "\n",
        "            # Treinar crítico / Train critical\n",
        "            for _ in range(5):  # n_critic steps\n",
        "                z = torch.randn(cur_batch_size, noise_dim).to(device)\n",
        "                fake = gen(z)\n",
        "\n",
        "                critic_real = critic(real).mean()\n",
        "                critic_fake = critic(fake).mean()\n",
        "                gp = gradient_penalty(critic, real, fake, device=device)\n",
        "                loss_critic = -(critic_real - critic_fake) + 10 * gp\n",
        "\n",
        "                opt_critic.zero_grad()\n",
        "                loss_critic.backward(retain_graph=True)\n",
        "                opt_critic.step()\n",
        "\n",
        "            # Treinar gerador / Train generator\n",
        "            z = torch.randn(cur_batch_size, noise_dim).to(device)\n",
        "            fake = gen(z)\n",
        "            gen_loss = -critic(fake).mean()\n",
        "\n",
        "            opt_gen.zero_grad()\n",
        "            gen_loss.backward()\n",
        "            opt_gen.step()\n",
        "\n",
        "    # ===== Gerar novos espectros / Generate new spectra =====\n",
        "    z = torch.randn(n_fake, noise_dim).to(device)\n",
        "    fake = gen(z).detach().cpu().numpy()\n",
        "    return fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I6yf_oWzkxuj"
      },
      "outputs": [],
      "source": [
        "# ========= 5. Data augmentation para todas as classes / Data augmentation for all classes =========\n",
        "classes_unicas = np.unique(classes)\n",
        "dados_sinteticos = []\n",
        "\n",
        "for c in classes_unicas:\n",
        "    X_c = X_scaled[classes == c]\n",
        "    espectros_fake = treinar_wgan(X_c, n_epochs=500, n_fake=500)\n",
        "    # Desnormalizar / Denormalize\n",
        "    espectros_fake = scaler.inverse_transform(espectros_fake)\n",
        "    # Reconstituir com a classe / Reconstitute with the class\n",
        "    for ef in espectros_fake:\n",
        "        dados_sinteticos.append([c] + ef.tolist())\n",
        "\n",
        "# Concatenar originais + sintéticos / Concatenate originals + synthetics\n",
        "dados_finais = pd.DataFrame(\n",
        "    [[c] + x.tolist() for c, x in zip(classes, scaler.inverse_transform(X_scaled))] + dados_sinteticos,\n",
        "    columns=[\"Classe\"] + list(features)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43gKxMS9ky-q",
        "outputId": "f3bc825a-e24b-4930-a9f3-1fbd07207c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arquivo salvo em dataset_augmented.csv\n"
          ]
        }
      ],
      "source": [
        "# ========= 6. Salvar / save =========\n",
        "dados_finais.to_csv(\"dataset_augmented.csv\", index=False)\n",
        "print(\"Arquivo salvo em / File saved in dataset_augmented.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMbrpQ0aeLWAObR7VHqoMi+"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}