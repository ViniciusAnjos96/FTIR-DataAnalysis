{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfTpZLJH8wmK989ZPJfBid"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, label_binarize\n",
        "from scipy.stats import uniform, randint\n"
      ],
      "metadata": {
        "id": "BK76D7LK2Gex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification / classificação"
      ],
      "metadata": {
        "id": "nKPSG7Sl91sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o arquivo Excel / Load the Excel file\n",
        "try:\n",
        "    df = pd.read_excel('NAME.xlsx')\n",
        "    print(\"NAME.xlsx file loaded successfully!\")\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    display(df.head())\n",
        "    print(\"\\nDataFrame Information:\")\n",
        "    df.info()\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'NAME.xlsx' was not found. Please ensure the file name is correct and it is in the correct directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the file: {e}\")"
      ],
      "metadata": {
        "id": "DEl4L4Aa3DId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d606890"
      },
      "source": [
        "# Carregar o arquivo CSV\n",
        "try:\n",
        "    df = pd.read_csv('NAME.csv')\n",
        "    print(\"NAME.csv file loaded successfully!\")\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    display(df.head())\n",
        "    print(\"\\nDataFrame Information:\")\n",
        "    df.info()\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'NAME.xlsx' was not found. Please ensure the file name is correct and it is in the correct directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the file: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defina a coluna alvo pelo índice (substitua '0' pelo índice correto da sua coluna alvo) / Define the target column by its index (replace '0' with the correct index of your target column)\n",
        "target_column_index = 0\n",
        "target_column_name = df.columns[target_column_index]\n",
        "\n",
        "# Separação das features (X) e da variável alvo (y) / Separation of features (X) and target variable (y)\n",
        "X = df.drop(columns=[target_column_name])\n",
        "y = df[target_column_name]"
      ],
      "metadata": {
        "id": "ofzG5fx42ZhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificar a variável alvo se for categórica (Label Encoding) / Encode the target variable if it is categorical (Label Encoding)\n",
        "# Isso é necessário para que o XGBoost possa trabalhar com classes não numéricas / This is necessary for XGBoost to work with non-numeric classes\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Divisão dos dados em conjuntos de treinamento e teste / Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")"
      ],
      "metadata": {
        "id": "BeoZbTtw2eLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialização e treinamento do modelo XGBoost / Initialize and train the XGBoost model\n",
        "model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(label_encoder.classes_), eval_metric='mlogloss', random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "8kDIc-Qp2hb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T7FNUv61cJN"
      },
      "outputs": [],
      "source": [
        "# Previsões no conjunto de teste / Predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Avaliação do modelo / Model evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "\n",
        "# Visualização da Matriz de Confusão como Heatmap / Confusion Matrix Visualization as Heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predictions')\n",
        "plt.ylabel('True Values')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Opcional: Para ter uma ideia da importância das features / Optional: To get an idea of feature importance\n",
        "feature_importances = model.feature_importances_\n",
        "features = X.columns\n",
        "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n",
        "print(\"\\nFeature Importance (top 10):\")\n",
        "print(importance_df.head(10))"
      ],
      "metadata": {
        "id": "CF6VanIa2mMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Fold Cross Validation\n",
        "n_splits = 5 # Você pode alterar o número de folds / You can change the number of folds\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "accuracy_scores = []\n",
        "reports = []\n",
        "confusion_matrices = []\n",
        "\n",
        "print(f\"Starting K-Fold Cross Validation with {n_splits} folds...\")\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y_encoded)):\n",
        "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
        "    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y_encoded[train_index], y_encoded[test_index]\n",
        "\n",
        "    model_kf = xgb.XGBClassifier(objective='multi:softmax', num_class=len(label_encoder.classes_), eval_metric='mlogloss', random_state=42)\n",
        "    model_kf.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    y_pred_fold = model_kf.predict(X_test_fold)\n",
        "\n",
        "    accuracy_fold = accuracy_score(y_test_fold, y_pred_fold)\n",
        "    accuracy_scores.append(accuracy_fold)\n",
        "\n",
        "    report_fold = classification_report(y_test_fold, y_pred_fold, target_names=label_encoder.classes_)\n",
        "    reports.append(report_fold)\n",
        "\n",
        "    conf_matrix_fold = confusion_matrix(y_test_fold, y_pred_fold)\n",
        "    confusion_matrices.append(conf_matrix_fold)\n",
        "\n",
        "    print(f\"Accuracy of Fold {fold+1}: {accuracy_fold:.4f}\")\n",
        "\n",
        "print(f\"\\n--- Final Results of K-Fold Cross Validation (Average of {n_splits} Folds) ---\")\n",
        "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
        "print(f\"Standard Deviation of Accuracy: {np.std(accuracy_scores):.4f}\")"
      ],
      "metadata": {
        "id": "QjbY8tvq32Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obter as probabilidades previstas para cada classe / Get predicted probabilities for each class\n",
        "y_proba = model.predict_proba(X_test)\n",
        "\n",
        "# Número de classes / Number of classes\n",
        "n_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Binarizar as classes verdadeiras para o formato One-vs-Rest / Binarize true classes to One-vs-Rest format\n",
        "# NOTA: Para classificação binária, label_binarize pode retornar um array com apenas uma coluna.\n",
        "#       Para evitar IndexError ao plotar ROC, ajustaremos y_true dentro do loop.\n",
        "# NOTE: For binary classification, label_binarize might return an array with only one column.\n",
        "#       To avoid IndexError when plotting ROC, we will adjust y_true inside the loop.\n",
        "y_test_binarized = label_binarize(y_test, classes=range(n_classes))\n",
        "\n",
        "# Plotar a curva ROC para cada classe / Plot the ROC curve for each class\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(n_classes):\n",
        "    # Para cada classe, y_true deve ser 1 se a amostra pertencer à classe i, e 0 caso contrário.\n",
        "    # For each class, y_true should be 1 if the sample belongs to class i, and 0 otherwise.\n",
        "    fpr, tpr, _ = roc_curve((y_test == i).astype(int), y_proba[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'ROC curve of class {label_encoder.classes_[i]} (area = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2) # Linha pontilhada de referência (classificador aleatório) / Reference dashed line (random classifier)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve - One-vs-Rest')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RJvt2nJJ4oPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9194888b"
      },
      "source": [
        "# As variáveis y_test_binarized, y_proba e n_classes já foram calculadas e estão disponíveis / The variables y_test_binarized, y_proba and n_classes have already been calculated and are available\n",
        "\n",
        "# Calcular a curva ROC macro-média / Calculate the macro-average ROC curve\n",
        "# Primeiro, obtenha FPR e TPR para cada classe / First, get FPR and TPR for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc_per_class = []\n",
        "for i in range(n_classes):\n",
        "    # Para cada classe, y_true deve ser 1 se a amostra pertencer à classe i, e 0 caso contrário.\n",
        "    # For each class, y_true should be 1 if the sample belongs to class i, and 0 otherwise.\n",
        "    fpr[i], tpr[i], _ = roc_curve((y_test == i).astype(int), y_proba[:, i])\n",
        "    roc_auc_per_class.append(auc(fpr[i], tpr[i]))\n",
        "\n",
        "# Agregação de todas as taxas de falsos positivos (FPR) / Aggregation of all False Positive Rates (FPR)\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "\n",
        "# Interpolar todas as curvas ROC neste conjunto de pontos / Interpolate all ROC curves at this set of points\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "# Finalmente, a média e a área / Finally, the average and the area\n",
        "mean_tpr /= n_classes\n",
        "\n",
        "fpr_macro = all_fpr\n",
        "tpr_macro = mean_tpr\n",
        "roc_auc_macro = auc(fpr_macro, tpr_macro)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_macro, tpr_macro, lw=2, label=f'Macro-average ROC curve (area = {roc_auc_macro:.2f})', color='navy', linestyle='--')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2) # Linha pontilhada de referência (classificador aleatório) / Reference dashed line (random classifier)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve - Macro-Average')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressão / regression"
      ],
      "metadata": {
        "id": "U3sN8Y7YAWAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sem label / without label"
      ],
      "metadata": {
        "id": "oDbObfnMAisi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o arquivo Excel / Load the Excel file\n",
        "try:\n",
        "    df = pd.read_excel('NAME.xlsx')\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'NAME.xlsx' was not found. Make sure it is in the correct directory.\")\n",
        "    # Você pode querer adicionar um código para upload aqui, se necessário. / You might want to add upload code here, if necessary.\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the file: {e}\")"
      ],
      "metadata": {
        "id": "pW0R2SxmFkEk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o arquivo CSV / Load the CSV file\n",
        "try:\n",
        "    df = pd.read_csv('NAME.csv')\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'NAME.csv' was not found. Make sure it is in the correct directory.\")\n",
        "    # Você pode querer adicionar um código para upload aqui, se necessário. / You might want to add upload code here, if necessary.\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the file: {e}\")"
      ],
      "metadata": {
        "id": "v8Oe8BwwCfr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supondo que a primeira coluna seja a variável alvo (y) e as outras sejam as features (X) / Assuming the first column is the target variable (y) and the others are features (X)\n",
        "X = df.iloc[:, 1:] # Todas as colunas exceto a primeira / All columns except the first\n",
        "y = df.iloc[:, 0]  # A primeira coluna / The first column\n",
        "\n",
        "print(f\"Target column name: {y.name}\")\n",
        "\n",
        "# Converter colunas de objetos para numéricas em X, se possível. / Convert object columns to numeric in X, if possible.\n",
        "for col in X.select_dtypes(include=['object']).columns:\n",
        "    try:\n",
        "        X[col] = pd.to_numeric(X[col])\n",
        "    except ValueError:\n",
        "        print(f\"Warning: Column '{col}' in X could not be converted to numeric and will be ignored or require specific pre-processing.\")\n",
        "\n",
        "# Converter y para numérica, se for objeto / Convert y to numeric, if it is an object\n",
        "if y.dtype == 'object':\n",
        "    try:\n",
        "        y = pd.to_numeric(y)\n",
        "    except ValueError:\n",
        "        print(\"Warning: The target variable (y) could not be converted to numeric. XGBoost requires a numeric target for regression.\")\n",
        "        print(\"Please check if your target column contains only numeric values or if it needs different pre-processing.\")\n",
        "\n",
        "# **Nova etapa de limpeza robusta:** / **New robust cleaning step:**\n",
        "# Remover NaN e valores infinitos de X e y, mantendo a correspondência entre as linhas / Remove NaN and infinite values from X and y, maintaining correspondence between rows\n",
        "initial_rows = X.shape[0]\n",
        "\n",
        "# Substituir valores infinitos por NaN em X e y / Replace infinite values with NaN in X and y\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "y.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Criar uma máscara para linhas com qualquer NaN em X ou y / Create a mask for rows with any NaN in X or y\n",
        "nan_mask_X = X.isnull().any(axis=1)\n",
        "nan_mask_y = y.isnull()\n",
        "\n",
        "# Combinar máscaras: manter linhas onde nem X nem y têm NaN / Combine masks: keep rows where neither X nor y have NaN\n",
        "valid_rows_mask = ~(nan_mask_X | nan_mask_y)\n",
        "\n",
        "X = X[valid_rows_mask]\n",
        "y = y[valid_rows_mask]\n",
        "\n",
        "if X.shape[0] < initial_rows:\n",
        "    print(f\"Warning: {initial_rows - X.shape[0]} rows were removed due to NaN or infinite values in X or y.\")\n",
        "    # Verificar se X ou y ficaram vazios após a limpeza / Check if X or y became empty after cleaning\n",
        "    if X.empty or y.empty:\n",
        "        raise ValueError(\"After cleaning, X or y are empty. Cannot continue with training.\")\n",
        "\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste / Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "DnMp3igJCh4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation of parameters / avaliação de parametros.\n",
        "\n",
        "# For regression, we directly use y_train and y_test, no need for argmax.\n",
        "\n",
        "n_estimators_range = range(1, 101, 5) # Increased range for n_estimators, adjusted step\n",
        "train_rmse_scores = []\n",
        "test_rmse_scores = []\n",
        "\n",
        "for n_est in n_estimators_range:\n",
        "    # Initialize the XGBoost Regressor for evaluation\n",
        "    model_eval = xgb.XGBRegressor(\n",
        "        objective='reg:squarederror', # Objective for regression problems\n",
        "        n_estimators=n_est,          # Number of trees (from the range)\n",
        "        learning_rate=0.1,           # Learning rate (can be adjusted)\n",
        "        random_state=42\n",
        "    )\n",
        "    model_eval.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions for training and test sets\n",
        "    y_train_pred = model_eval.predict(X_train)\n",
        "    y_test_pred = model_eval.predict(X_test)\n",
        "\n",
        "    # Calculate RMSE for training and test sets\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "    train_rmse_scores.append(train_rmse)\n",
        "    test_rmse_scores.append(test_rmse)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_estimators_range, train_rmse_scores, label='Training', marker='o')\n",
        "plt.plot(n_estimators_range, test_rmse_scores, label='Validation', marker='o')\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('RMSE')\n",
        "plt.title('XGBoost Regression: RMSE vs. Number of Estimators')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the n_estimators with the minimum validation RMSE\n",
        "min_rmse = min(test_rmse_scores)\n",
        "min_rmse_index = test_rmse_scores.index(min_rmse)\n",
        "best_n_estimators = n_estimators_range[min_rmse_index]\n",
        "\n",
        "print(f\"Number of Estimators with the minimum validation RMSE: {best_n_estimators}\")\n",
        "print(f\"Minimum validation RMSE: {min_rmse:.4f}\")"
      ],
      "metadata": {
        "id": "iR2v160BKmyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar o regressor XGBoost / Initialize the XGBoost regressor\n",
        "xgb_reg = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror', # Objetivo para problemas de regressão / Objective for regression problems\n",
        "    n_estimators=46,             # Número de árvores (ajustável) / Number of trees (adjustable)\n",
        "    learning_rate=0.1,            # Taxa de aprendizado (ajustável) / Learning rate (adjustable)\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Treinar o modelo / Train the model\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "\n",
        "print(\"XGBoost model trained successfully!\")"
      ],
      "metadata": {
        "id": "SfuY-tl1Cjab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazer previsões no conjunto de teste / Make predictions on the test set\n",
        "y_pred = xgb_reg.predict(X_test)\n",
        "\n",
        "# Avaliar o modelo / Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred);\n",
        "rmse = np.sqrt(mse);\n",
        "r2 = r2_score(y_test, y_pred);\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"R-squared (R²): {r2:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.7)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted Values (XGBoost Regression)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9zVajiN_ClEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the XGBoost Regressor model\n",
        "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 200),  # Number of boosting rounds\n",
        "    'learning_rate': uniform(0.01, 0.2), # Step size shrinkage to prevent overfitting\n",
        "    'max_depth': randint(3, 10),      # Maximum depth of a tree\n",
        "    'subsample': uniform(0.6, 0.4),   # Subsample ratio of the training instance\n",
        "    'colsample_bytree': uniform(0.6, 0.4), # Subsample ratio of columns when constructing each tree\n",
        "    'gamma': uniform(0, 0.5)          # Minimum loss reduction required to make a further partition on a leaf node\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "# n_iter: Number of parameter settings that are sampled. More is better but takes longer.\n",
        "# cv: Number of cross-validation folds.\n",
        "# scoring: Metric to evaluate the performance (e.g., 'neg_mean_squared_error' for regression)\n",
        "random_search = RandomizedSearchCV(estimator=model,\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=100, # You can adjust this number\n",
        "                                   cv=5,\n",
        "                                   scoring='neg_mean_squared_error',\n",
        "                                   verbose=1,\n",
        "                                   n_jobs=-1, # Use all available cores\n",
        "                                   random_state=42)\n",
        "\n",
        "# Fit RandomizedSearchCV to the data\n",
        "print(\"Starting RandomizedSearchCV...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"\\nBest parameters found: \", random_search.best_params_)\n",
        "print(\"Best RMSE found: \", np.sqrt(-random_search.best_score_))\n",
        "\n",
        "# Get the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
        "r2_best = r2_score(y_test, y_pred_best)\n",
        "\n",
        "print(f\"\\nRMSE of the best model on the test set: {rmse_best:.4f}\")\n",
        "print(f\"R-squared of the best model on the test set: {r2_best:.4f}\")\n"
      ],
      "metadata": {
        "id": "4Dd-Z8C8OU5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## com label / with label"
      ],
      "metadata": {
        "id": "W0yaw2vtAlue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o arquivo Excel / Load the Excel file\n",
        "try:\n",
        "    df = pd.read_excel('NAME.xlsx')\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'NAME.xlsx' was not found. Make sure it is in the correct directory.\")\n",
        "    # Você pode querer adicionar um código para upload aqui, se necessário. / You might want to add upload code here, if necessary.\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the file: {e}\")"
      ],
      "metadata": {
        "id": "RnMBDIZjFfeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o arquivo CSV / Load the CSV file\n",
        "try:\n",
        "    df = pd.read_csv('NAME.csv')\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'NAME.csv' was not found. Make sure it is in the correct directory.\")\n",
        "    # Você pode querer adicionar um código para upload aqui, se necessário. / You might want to add upload code here, if necessary.\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the file: {e}\")"
      ],
      "metadata": {
        "id": "dsZ4r7G5FJx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supondo que a primeira coluna do dataframe deve ser excluída e o novo primeiro coluna será a variável alvo (y) e as outras serão as features (X) / Assuming the first column of the dataframe should be excluded and the new first column will be the target variable (y) and the others will be the features (X)\n",
        "df_processed = df.drop(columns=[df.columns[0]]) # Exclui a primeira coluna do dataframe / Excludes the first column from the dataframe\n",
        "df_processed.head()"
      ],
      "metadata": {
        "id": "u6_JIEwFFKbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df_processed.iloc[:, 0]  # A nova primeira coluna / The new first column\n",
        "X = df_processed.iloc[:, 1:] # Todas as outras colunas / All other columns\n",
        "\n",
        "print(f\"Target column name: {y.name}\")\n",
        "\n",
        "# Converter colunas de objetos para numéricas em X, se possível. / Convert object columns to numeric in X, if possible.\n",
        "for col in X.select_dtypes(include=['object']).columns:\n",
        "    try:\n",
        "        X[col] = pd.to_numeric(X[col])\n",
        "    except ValueError:\n",
        "        print(f\"Warning: Column '{col}' in X could not be converted to numeric and will be ignored or require specific pre-processing.\")\n",
        "\n",
        "# Converter y para numérica, se for objeto (já deveria ser numérica aqui) / Convert y to numeric, if it is an object\n",
        "if y.dtype == 'object':\n",
        "    try:\n",
        "        y = pd.to_numeric(y)\n",
        "    except ValueError:\n",
        "        print(\"Warning: The target variable (y) could not be converted to numeric. XGBoost requires a numeric target for regression.\")\n",
        "        print(\"Please check if your target column contains only numeric values or if it needs different pre-processing.\")\n",
        "\n",
        "# **Nova etapa de limpeza robusta:** / **New robust cleaning step:**\n",
        "# Remover NaN e valores infinitos de X e y, mantendo a correspondência entre as linhas / Remove NaN and infinite values from X and y, maintaining correspondence between rows\n",
        "initial_rows = X.shape[0]\n",
        "\n",
        "# Substituir valores infinitos por NaN em X e y / Replace infinite values by NaN in X and y\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "y.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Criar uma máscara para linhas com qualquer NaN em X ou y / Create a mask for rows with any NaN in X or y\n",
        "nan_mask_X = X.isnull().any(axis=1)\n",
        "nan_mask_y = y.isnull()\n",
        "\n",
        "# Combinar máscaras: manter linhas onde nem X nem y têm NaN / Combine masks: keep rows where neither X nor y have NaN\n",
        "valid_rows_mask = ~(nan_mask_X | nan_mask_y)\n",
        "\n",
        "X = X[valid_rows_mask]\n",
        "y = y[valid_rows_mask]\n",
        "\n",
        "if X.shape[0] < initial_rows:\n",
        "    print(f\"Warning: {initial_rows - X.shape[0]} rows were removed due to NaN or infinite values in X or y.\")\n",
        "    # Verificar se X ou y ficaram vazios após a limpeza / Check if X or y became empty after cleaning\n",
        "    if X.empty or y.empty:\n",
        "        raise ValueError(\"After cleaning, X or y are empty. Cannot continue with training.\")\n",
        "\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste / Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "TVXtfkTXFNG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation of parameters / avaliação de parametros.\n",
        "\n",
        "# For regression, we directly use y_train and y_test, no need for argmax.\n",
        "\n",
        "n_estimators_range = range(1, 101, 5) # Increased range for n_estimators, adjusted step\n",
        "train_rmse_scores = []\n",
        "test_rmse_scores = []\n",
        "\n",
        "for n_est in n_estimators_range:\n",
        "    # Initialize the XGBoost Regressor for evaluation\n",
        "    model_eval = xgb.XGBRegressor(\n",
        "        objective='reg:squarederror', # Objective for regression problems\n",
        "        n_estimators=n_est,          # Number of trees (from the range)\n",
        "        learning_rate=0.1,           # Learning rate (can be adjusted)\n",
        "        random_state=42\n",
        "    )\n",
        "    model_eval.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions for training and test sets\n",
        "    y_train_pred = model_eval.predict(X_train)\n",
        "    y_test_pred = model_eval.predict(X_test)\n",
        "\n",
        "    # Calculate RMSE for training and test sets\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "    train_rmse_scores.append(train_rmse)\n",
        "    test_rmse_scores.append(test_rmse)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_estimators_range, train_rmse_scores, label='Training', marker='o')\n",
        "plt.plot(n_estimators_range, test_rmse_scores, label='Validation', marker='o')\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('RMSE')\n",
        "plt.title('XGBoost Regression: RMSE vs. Number of Estimators')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the n_estimators with the minimum validation RMSE\n",
        "min_rmse = min(test_rmse_scores)\n",
        "min_rmse_index = test_rmse_scores.index(min_rmse)\n",
        "best_n_estimators = n_estimators_range[min_rmse_index]\n",
        "\n",
        "print(f\"Number of Estimators with the minimum validation RMSE: {best_n_estimators}\")\n",
        "print(f\"Minimum validation RMSE: {min_rmse:.4f}\")"
      ],
      "metadata": {
        "id": "HcIt5ra8KiQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar o regressor XGBoost / Initialize the XGBoost regressor\n",
        "xgb_reg = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror', # Objetivo para problemas de regressão / Objective for regression problems\n",
        "    n_estimators=96,             # Número de árvores (ajustável) / Number of trees (adjustable)\n",
        "    learning_rate=0.1,            # Taxa de aprendizado (ajustável) / Learning rate (adjustable)\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Treinar o modelo / Train the model\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "\n",
        "print(\"XGBoost model trained successfully!\")"
      ],
      "metadata": {
        "id": "7-0aaQ1IFPlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazer previsões no conjunto de teste / Make predictions on the test set\n",
        "y_pred = xgb_reg.predict(X_test)\n",
        "\n",
        "# Avaliar o modelo / Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred);\n",
        "rmse = np.sqrt(mse);\n",
        "r2 = r2_score(y_test, y_pred);\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"R-squared (R²): {r2:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.7)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted Values (XGBoost Regression)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TvSdkSXtFRA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the XGBoost Regressor model\n",
        "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 200),  # Number of boosting rounds\n",
        "    'learning_rate': uniform(0.01, 0.2), # Step size shrinkage to prevent overfitting\n",
        "    'max_depth': randint(3, 10),      # Maximum depth of a tree\n",
        "    'subsample': uniform(0.6, 0.4),   # Subsample ratio of the training instance\n",
        "    'colsample_bytree': uniform(0.6, 0.4), # Subsample ratio of columns when constructing each tree\n",
        "    'gamma': uniform(0, 0.5)          # Minimum loss reduction required to make a further partition on a leaf node\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "# n_iter: Number of parameter settings that are sampled. More is better but takes longer.\n",
        "# cv: Number of cross-validation folds.\n",
        "# scoring: Metric to evaluate the performance (e.g., 'neg_mean_squared_error' for regression)\n",
        "random_search = RandomizedSearchCV(estimator=model,\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=100, # You can adjust this number\n",
        "                                   cv=5,\n",
        "                                   scoring='neg_mean_squared_error',\n",
        "                                   verbose=1,\n",
        "                                   n_jobs=-1, # Use all available cores\n",
        "                                   random_state=42)\n",
        "\n",
        "# Fit RandomizedSearchCV to the data\n",
        "print(\"Starting RandomizedSearchCV...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"\\nBest parameters found: \", random_search.best_params_)\n",
        "print(\"Best RMSE found: \", np.sqrt(-random_search.best_score_))\n",
        "\n",
        "# Get the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
        "r2_best = r2_score(y_test, y_pred_best)\n",
        "\n",
        "print(f\"\\nRMSE of the best model on the test set: {rmse_best:.4f}\")\n",
        "print(f\"R-squared of the best model on the test set: {r2_best:.4f}\")\n"
      ],
      "metadata": {
        "id": "9LLQDgZUOCuU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}