{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6vU6abb4QLebA+S685s8S"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65R1O4X338_t"
      },
      "outputs": [],
      "source": [
        "# Partial Least Squares Discriminant Analysis\n",
        "# Change the NAME.csv\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Carregar o arquivo CSV\n",
        "try:\n",
        "    df = pd.read_csv('NAME.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Erro: Arquivo 'NAME.csv' não encontrado. Certifique-se de que o arquivo está na mesma pasta ou forneça o caminho completo.\")\n",
        "    # Você pode adicionar um código aqui para fazer upload do arquivo, se necessário.\n",
        "    # from google.colab import files\n",
        "    # uploaded = files.upload()\n",
        "    # df = pd.read_csv('NAME.csv')\n",
        "    exit() # Termina a execução se o arquivo não for encontrado\n",
        "\n",
        "# Separar os dados: A primeira coluna é a classe, o resto são os espectros\n",
        "X = df.iloc[:, 1:].values  # Dados dos espectros (todas as colunas, exceto a primeira)\n",
        "y = df.iloc[:, 0].values   # Classes (primeira coluna)\n",
        "wavenumbers = df.columns[1:].astype(float) # A primeira linha (coluna 1 em diante) são os números de onda\n",
        "\n",
        "# Converter as classes para formato numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Dividir os dados em conjunto de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Criar o modelo PLS-DA\n",
        "# O número de componentes é um hiperparâmetro que pode ser otimizado\n",
        "n_components = min(X_train.shape[0], X_train.shape[1], 10) # Exemplo: usar até 10 componentes ou o mínimo possível\n",
        "plsda = PLSRegression(n_components=n_components)\n",
        "\n",
        "# Treinar o modelo\n",
        "plsda.fit(X_train, y_train)\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred_raw = plsda.predict(X_test)\n",
        "\n",
        "# Para classificação, PLS-DA geralmente requer um passo adicional para converter as saídas contínuas em classes.\n",
        "# Uma abordagem comum é usar um classificador nos scores latentes ou binarizar as previsões.\n",
        "# Uma maneira simples para múltiplas classes é atribuir a classe com a maior pontuação (assumindo que o LabelEncoder mapeou as classes sequencialmente).\n",
        "# Uma abordagem mais robusta seria usar um classificador (como Logistic Regression ou SVM) nos scores latentes.\n",
        "\n",
        "# Abordagem simples: arredondar e clipar para os limites das classes codificadas\n",
        "y_pred_rounded = np.round(y_pred_raw).astype(int)\n",
        "y_pred = np.clip(y_pred_rounded, 0, len(label_encoder.classes_) - 1) # Garante que as previsões estejam dentro dos limites das classes codificadas\n",
        "\n",
        "# Avaliar o modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Acurácia do modelo PLS-DA: {accuracy:.2f}')\n",
        "\n",
        "print('\\nRelatório de Classificação:')\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Visualizar os resultados do PLS-DA (opcional)\n",
        "# Scores latentes\n",
        "X_train_r, Y_train_r = plsda.transform(X_train, y_train)\n",
        "X_test_r, Y_test_r = plsda.transform(X_test, y_test)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k'] # Cores para as classes\n",
        "\n",
        "for i, target_name in enumerate(label_encoder.classes_):\n",
        "    plt.scatter(X_test_r[y_test == i, 0], X_test_r[y_test == i, 1], color=colors[i % len(colors)], label=target_name)\n",
        "\n",
        "plt.xlabel(f'Componente Latente 1 ({plsda.x_scores_[:, 0].var():.2f}% variância explicada)')\n",
        "plt.ylabel(f'Componente Latente 2 ({plsda.x_scores_[:, 1].var():.2f}% variância explicada)')\n",
        "plt.title('PLS-DA: Scores Latentes dos Dados de Teste')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Coeficientes de regressão do PLS-DA (mostra a importância das variáveis - números de onda)\n",
        "# Note que em PLS-DA, a interpretação dos coeficientes pode ser complexa.\n",
        "# Muitas vezes, os \"loadings\" ou \"VIP scores\" (Variable Importance in Projection) são usados para identificar variáveis importantes.\n",
        "# O sklearn PLSRegression não calcula VIP scores diretamente. Podemos usar os loadings.\n",
        "\n",
        "# Loadings X\n",
        "# loadings_x = plsda.x_loadings_\n",
        "\n",
        "# Coeficientes de regressão (relação entre X e Y latentes)\n",
        "# regression_coefficients = plsda.coef_\n",
        "\n",
        "# Se você quiser plotar algo que se relacione com a importância dos números de onda,\n",
        "# pode plotar os loadings do primeiro componente latente, por exemplo.\n",
        "# Loadings altos (positivos ou negativos) neste componente indicam que este número de onda contribui significativamente para essa componente.\n",
        "\n",
        "if plsda.x_loadings_.shape[1] > 0:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(wavenumbers, plsda.x_loadings_[:, 0])\n",
        "    plt.xlabel('Número de Onda (cm⁻¹)')\n",
        "    plt.ylabel('Loading do Componente 1')\n",
        "    plt.title('Loadings do Primeiro Componente PLS-DA')\n",
        "    plt.gca().invert_xaxis() # Números de onda geralmente são plotados em ordem decrescente\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Não foi possível calcular/plotar os loadings, verifique o número de componentes.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PLS-DA with confidence elipse, english.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.patches import Ellipse\n",
        "import matplotlib.transforms as transforms\n",
        "\n",
        "def confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', **kwargs):\n",
        "    \"\"\"\n",
        "    Create a plot of the covariance confidence ellipse of `x` and `y`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x, y : array_like, shape (n, ).\n",
        "        Input data.\n",
        "    ax : matplotlib.axes.Axes\n",
        "        The axes object to draw the ellipse into.\n",
        "    n_std : float\n",
        "        The number of standard deviations to determine the ellipse's size.\n",
        "    facecolor : str\n",
        "        Color of the ellipse face. Defaults to 'none'.\n",
        "    kwargs : dict\n",
        "        Forwarded to `~matplotlib.patches.Ellipse`\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib.patches.Ellipse\n",
        "    \"\"\"\n",
        "    if x.size != y.size:\n",
        "        raise ValueError(\"x and y must be the same size\")\n",
        "\n",
        "    cov = np.cov(x, y)\n",
        "    pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n",
        "    # Using a special case to obtain the eigenvalues of a symmetric\n",
        "    # positive definite matrix in two dimensions\n",
        "    ell_radius_x = np.sqrt(1 + pearson)\n",
        "    ell_radius_y = np.sqrt(1 - pearson)\n",
        "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
        "                      facecolor=facecolor, **kwargs)\n",
        "\n",
        "    # Calculating the stdandard deviation of x from the squareroot of the variance and multiplying\n",
        "    # with the given number of standard deviations.\n",
        "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
        "    mean_x = np.mean(x)\n",
        "\n",
        "    # Calculating the stdandard deviation of y from the squareroot of the variance and multiplying\n",
        "    # with the given number of standard deviations.\n",
        "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
        "    mean_y = np.mean(y)\n",
        "\n",
        "    transf = transforms.Affine2D().rotate_deg(45).scale(scale_x, scale_y).translate(mean_x, mean_y)\n",
        "\n",
        "    ellipse.set_transform(transf + ax.transData)\n",
        "    return ax.add_patch(ellipse)\n",
        "\n",
        "\n",
        "# Calculate latent scores for the full dataset (train + test) for better ellipse visualization\n",
        "X_full_r, Y_full_r = plsda.transform(X, y_encoded)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k'] # Colors for the classes\n",
        "\n",
        "ax = plt.gca() # Get current axes\n",
        "\n",
        "for i, target_name in enumerate(label_encoder.classes_):\n",
        "    # Plot the points\n",
        "    ax.scatter(X_full_r[y_encoded == i, 0], X_full_r[y_encoded == i, 1],\n",
        "               color=colors[i % len(colors)], label=target_name, alpha=0.6)\n",
        "\n",
        "    # Plot the confidence ellipse\n",
        "    confidence_ellipse(X_full_r[y_encoded == i, 0], X_full_r[y_encoded == i, 1], ax,\n",
        "                       n_std=2.0, edgecolor=colors[i % len(colors)], linestyle='--', alpha=0.5) # 2 std for ~95% confidence\n",
        "\n",
        "plt.xlabel(f'Latent Component 1 (Explained Variance: {plsda.x_scores_[:, 0].var():.2f}%)')\n",
        "plt.ylabel(f'Latent Component 2 (Explained Variance: {plsda.x_scores_[:, 1].var():.2f}%)')\n",
        "plt.title('PLS-DA: Latent Scores with Confidence Ellipses')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zQhu4nbZ5LZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}