{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4rsgHwDT8y2DUdVHBFK0P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===== Pacotes / Packages =====\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, r2_score, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Fixar seed para reprodutibilidade / Fix seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "gYihXeEANhyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Carregar dados .xlsx / Load data .xlsx =====\n",
        "df = pd.read_excel(\"NAME.xlsx\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "UEdbKqbmNm3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Carregar dados .csv / Load data .csv =====\n",
        "df = pd.read_csv(\"NAME.csv\", sep=',') # especifica o separador / specifies the separator\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "7ID1osChvNPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar features (X) e target (y) / Separate features (X) and target (y)\n",
        "X = df.iloc[:, 1:].values   # todas as colunas menos a primeira / all columns except the first\n",
        "y = df.iloc[:, 0].values    # apenas a primeira coluna / just the first column\n",
        "\n",
        "# Se 'y' contiver strings, codificar os rótulos / If 'y' contains strings, encode the labels\n",
        "if y.dtype == 'object': # ou use isinstance(y[0], str) se preferir verificar um elemento / or use isinstance(y[0], str) if you prefer to check one element\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "else:\n",
        "    y_encoded = y\n",
        "\n",
        "# Transformar y em categorias (one-hot encoding) / Transform y into categories (one-hot encoding)\n",
        "# Use y_encoded para one-hot encoding / Use y_encoded for one-hot encoding\n",
        "y_cat = to_categorical(y_encoded)\n",
        "\n",
        "# Dividir em treino e teste / Split into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_cat, test_size=0.2, random_state=42, stratify=y_encoded # stratify com y_encoded\n",
        ")\n",
        "\n",
        "# Normalizar dados / Normalize data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "utATycwvNsXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Fit - Classificação/Classification\n",
        "Define o número de neurônios na camada oculta e o número de épocas ideal com base no MSE. Utiliza 'cross entropy' como função de perda / Sets the number of neurons in the hidden layer and the optimal number of epochs based on the MSE. Uses cross entropy as the loss function."
      ],
      "metadata": {
        "id": "8-XC5XfXMolG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Função para criar o modelo / Function to create the model =====\n",
        "def criar_modelo(num_neuronios, input_dim, output_dim):\n",
        "    model = Sequential([\n",
        "        Dense(num_neuronios, input_dim=input_dim, activation=\"relu\"),\n",
        "        Dense(output_dim, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "Wgt3mT4wMtrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Avaliação combinada: neurônios X épocas / Combined evaluation: neurons X epochs =====\n",
        "\n",
        "neuron_range = [2, 4, 8, 16, 32, 64] # quantidade de neurônios a ser testados / number of neurons to be tested\n",
        "epoch_range = list(range(5, 105, 5))  # de 'início' a 'final' com 'passo' / from 'start' to 'end' with 'step'\n",
        "mse_results = {n: [] for n in neuron_range}\n",
        "melhor_mse = float(\"inf\")\n",
        "melhor_comb = (None, None)\n",
        "\n",
        "for n in neuron_range:\n",
        "    print(f\"\\n>> Neurons: {n}\")\n",
        "    for ep in epoch_range:\n",
        "        model = criar_modelo(n, X.shape[1], y_cat.shape[1])\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_split=0.2,\n",
        "            epochs=ep,\n",
        "            batch_size=16,\n",
        "            verbose=0\n",
        "        )\n",
        "        y_pred = model.predict(X_test)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        mse_results[n].append(mse)\n",
        "\n",
        "        print(f\"   Épocas: {ep} → MSE: {mse:.4f}\")\n",
        "\n",
        "        if mse < melhor_mse:\n",
        "            melhor_mse = mse\n",
        "            melhor_comb = (n, ep)\n",
        "\n",
        "# ===== Plot MSE vs Épocas / Plotar MSE vs Épocas =====\n",
        "plt.figure(figsize=(10,6))\n",
        "for n in neuron_range:\n",
        "    plt.plot(epoch_range, mse_results[n], marker='o', label=f'{n} neurons')\n",
        "\n",
        "plt.title(\"Mean Squared Error (MSE) vs Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"MSE on the test set\")\n",
        "plt.legend(title=\"Hidden layer\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-7zYjhv3MxAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Resultado melhor combinação / Best combination result =====\n",
        "melhor_n, melhor_ep = melhor_comb\n",
        "print(f\"\\nBest configuration: {melhor_n} neurons and {melhor_ep} epochs (MSE: {melhor_mse:.4f})\")"
      ],
      "metadata": {
        "id": "B7VZUZ8QMzg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Treinar modelo final com melhor combinação / Train final model with best combination =====\n",
        "model = criar_modelo(melhor_n, X.shape[1], y_cat.shape[1])\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=melhor_ep,\n",
        "    batch_size=16,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Avaliar modelo final / Evaluate final model\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest accuracy: {acc:.2f}\")"
      ],
      "metadata": {
        "id": "56y-dit8M06F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training history for classification / Traçando o histórico de treinamento para classificação\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.title('Loss Curve (Cross Entropy) - Classification')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "plt.title('Accuracy Curve - Classification')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KFz98U6RL0nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Mostrar e salvar pesos aprendidos / Show and save learned weights =====\n",
        "for i, layer in enumerate(model.layers):\n",
        "    weights, biases = layer.get_weights()\n",
        "    layer_name = layer.name\n",
        "\n",
        "    print(f\"\\nLayer {i+1} - {layer_name}\")\n",
        "    print(\"Weights (W):\")\n",
        "    print(weights)\n",
        "    print(\"Biases (b):\")\n",
        "    print(biases)\n",
        "\n",
        "    # Salvar Pesos / Save Weights\n",
        "    weights_df = pd.DataFrame(weights)\n",
        "    weights_csv_filename = f\"weights_{layer_name}.csv\"\n",
        "    weights_excel_filename = f\"weights_{layer_name}.xlsx\"\n",
        "\n",
        "    weights_df.to_csv(weights_csv_filename, index=False)\n",
        "    weights_df.to_excel(weights_excel_filename, index=False)\n",
        "    print(f\"Layer weights {layer_name} saved in {weights_csv_filename} and {weights_excel_filename}\")\n",
        "\n",
        "    # Salvar Biases (verificar se a camada tem biases) / Save Biases (check if the layer has biases)\n",
        "    if len(biases) > 0:\n",
        "        biases_df = pd.DataFrame(biases, columns=['Bias'])\n",
        "        biases_csv_filename = f\"biases_{layer_name}.csv\"\n",
        "        biases_excel_filename = f\"biases_{layer_name}.xlsx\"\n",
        "\n",
        "        biases_df.to_csv(biases_csv_filename, index=False)\n",
        "        biases_df.to_excel(biases_excel_filename, index=False)\n",
        "        print(f\"Layer biases {layer_name} saved in {biases_csv_filename} and {biases_excel_filename}\")\n",
        "    else:\n",
        "        print(f\"Layer {layer_name} has no biases.\")"
      ],
      "metadata": {
        "id": "HxsvxfKZgBwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificação / Classification"
      ],
      "metadata": {
        "id": "M6d7T-5KMrm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Definir modelo MLP / Define MLP Model =====\n",
        "neurons = 2  # número de neurônios na camada oculta / number of neurons in the hidden layer\n",
        "model = Sequential([\n",
        "    Dense(neurons, input_dim=X.shape[1], activation=\"relu\"), #input_dim=X.shape[1] diz à rede quantas entradas cada neurônio da primeira camada deve esperar, só é necessário na primeira camada / input_dim=X.shape[1] tells the network how many inputs each neuron in the first layer should expect, only needed in the first layer.\n",
        "    Dense(y_cat.shape[1], activation=\"softmax\") # camada de saida / output layer\n",
        "])"
      ],
      "metadata": {
        "id": "V37VbTdcOBkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilar modelo / compile model\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Treinar modelo / Train model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Avaliar modelo / Evaluate model\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {acc:.2f}\")"
      ],
      "metadata": {
        "id": "GowopTW5ODeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Matriz de confusão / Confusion matrix =====\n",
        "# Previsões / Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Matriz / Matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=np.unique(y),\n",
        "            yticklabels=np.unique(y))\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.show()\n",
        "\n",
        "# ===== Relatório de classificação / classification report =====\n",
        "print(\"Classification report:\\n\")\n",
        "print(classification_report(y_true, y_pred_classes, target_names=[f\"Class {c}\" for c in np.unique(y)]))"
      ],
      "metadata": {
        "id": "jBeDOpXiOHNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use curvas individuais (multiclass ROC) quando quiser analisar o desempenho do modelo em cada classe separadamente. / Use curvas individuais (ROC multiclasse) quando quiser analisar o desempenho do modelo em cada classe separadamente.\n",
        "\n",
        "# Fit LabelEncoder to the original unique classes / Ajustar LabelEncoder às classes originais exclusivas\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(np.unique(y))\n",
        "\n",
        "# Compute ROC curve and ROC area for each class / Calcular a curva ROC e a área ROC para cada classe\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "unique_classes = np.unique(y)  # Use original unique classes for labeling / Use classes originais exclusivas para rotulagem\n",
        "n_classes = len(unique_classes)\n",
        "\n",
        "for i in range(n_classes):\n",
        "    class_value = unique_classes[i]\n",
        "    # Use label_encoder to get the correct index for the current class / Use label_encoder para obter o índice correto para a classe atual\n",
        "    class_index = label_encoder.transform([class_value])[0]\n",
        "\n",
        "    # Check if the current class is present in the test set / Verifique se a classe atual está presente no conjunto de teste\n",
        "    if np.sum(y_test[:, class_index]) > 0:\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test[:, class_index], y_pred[:, class_index])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    else:\n",
        "        # If a class is not in the test set, set AUC to NaN and skip plotting its curve / Se uma classe não estiver no conjunto de teste, defina AUC como NaN e pule a plotagem de sua curva\n",
        "        roc_auc[i] = np.nan\n",
        "        print(f\"Warning: Class {class_value} has no positive samples in the test set. Skipping ROC curve for this class.\")\n",
        "\n",
        "\n",
        "# Plot ROC curves / Traçar curvas ROC\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
        "for i in range(n_classes):\n",
        "    # Only plot if the AUC was calculated (class was in the test set) / Somente plote se a AUC foi calculada (a classe estava no conjunto de teste)\n",
        "    if not np.isnan(roc_auc[i]):\n",
        "        # Ensure there are enough colors / Certifique-se de que há cores suficientes\n",
        "        color = colors[i % len(colors)]\n",
        "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "                 label='ROC curve of class {0} (area = {1:0.2f})'.format(unique_classes[i], roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([-0.05, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Multiclass ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4W2i1MeWOrZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25613d59"
      },
      "source": [
        "# Use macro-average ROC quando quiser uma visão geral do desempenho do modelo em todas as classes, sem considerar desequilíbrio entre elas. / Use ROC macro-médio quando quiser uma visão geral do desempenho do modelo em todas as classes, sem considerar desequilíbrio entre elas.\n",
        "\n",
        "# Calculate macro-average ROC curve and AUC / Calcular a curva ROC macromédia e a AUC\n",
        "# First aggregate all false positive rates / Primeiro, agregue todas as taxas de falsos positivos\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes) if not np.isnan(roc_auc[i])]))\n",
        "\n",
        "# Then interpolate all ROC curves at this points / Em seguida, interpole todas as curvas ROC nesses pontos\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    if not np.isnan(roc_auc[i]):\n",
        "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "# Average it and compute AUC / Calcule a média e a AUC\n",
        "mean_tpr /= sum([not np.isnan(roc_auc[i]) for i in range(n_classes)]) # Divide by the number of classes that were in the test set/ Divida pelo número de classes que estavam no conjunto de teste\n",
        "\n",
        "macro_roc_auc = auc(all_fpr, mean_tpr)\n",
        "\n",
        "# Plot macro-average ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(all_fpr, mean_tpr, color='red', linestyle='-', linewidth=2,\n",
        "         label='Macro-average ROC curve (area = {0:0.2f})'.format(macro_roc_auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([-0.05, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Multiclass ROC Curve (Macro-average)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Fit - Regressão / Regression\n",
        "Define o número de neurônios na camada oculta e o número de épocas ideal com base no MSE. Utiliza 'Mean Squared Error' como função de perda / Set the number of neurons in the hidden layer and the optimal number of epochs based on the MSE. Use 'Mean Squared Error' as the loss function."
      ],
      "metadata": {
        "id": "ZfAsdR5DQRWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_column_names = ['feature1', 'feature2'] # Change to a list of your desired target columns / Alterar para uma lista de suas colunas de destino desejadas\n",
        "\n",
        "# Check if all target columns exist in the DataFrame / Verifique se todas as colunas de destino existem no DataFrame\n",
        "if all(col in df.columns for col in target_column_names):\n",
        "    y_reg = df[target_column_names].values\n",
        "    # Define features for regression, excluding the classification target and regression targets / Definir recursos para regressão, excluindo o alvo de classificação e os alvos de regressão\n",
        "    # Assuming the first column is the classification target / Supondo que a primeira coluna seja o alvo da classificação\n",
        "    feature_columns_reg = [col for col in df.columns if col not in [df.columns[0]] + target_column_names]\n",
        "    X_reg = df[feature_columns_reg].values\n",
        "\n",
        "else:\n",
        "    missing_cols = [col for col in target_column_names if col not in df.columns]\n",
        "    print(f\"Columns {missing_cols} not found in the DataFrame. Please check the column names.\")\n",
        "    y_reg = None # Set y_reg to None if columns are missing / Defina y_reg como Nenhum se colunas estiverem faltando\n",
        "    X_reg = None # Also set X_reg to None if columns are missing / Defina também X_reg como Nenhum se colunas estiverem faltando\n",
        "\n",
        "\n",
        "# Split data for regression only if target columns were found / Dividir dados para regressão somente se colunas de destino forem encontradas\n",
        "if y_reg is not None and X_reg is not None:\n",
        "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "        X_reg, y_reg, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Normalize data for regression / Normalizar dados para regressão\n",
        "    scaler_reg = StandardScaler()\n",
        "    X_train_reg = scaler_reg.fit_transform(X_train_reg)\n",
        "    # We don't normalize y_reg for this type of regression / Não normalizamos y_reg para este tipo de regressão\n",
        "    X_test_reg = scaler_reg.transform(X_test_reg)"
      ],
      "metadata": {
        "id": "s37ZYKuuWNvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Função para criar o modelo / Function to create the model =====\n",
        "def criar_modelo_reg(num_neuronios, input_dim, output_dim):\n",
        "    model = Sequential([\n",
        "        Dense(num_neuronios, input_dim=input_dim, activation=\"relu\"),\n",
        "        Dense(output_dim, activation=\"linear\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "83rXzohKyqyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Avaliação combinada: neurônios X épocas / Combined evaluation: neurons X epoch =====\n",
        "\n",
        "neuron_range = [2, 4, 8, 16, 32, 64] # quantidade de neurônios a ser testados / number of neurons to be tested\n",
        "epoch_range = list(range(5, 105, 5))  # de 'início' a 'final' com 'passo' / from 'start' to 'end' with 'step'\n",
        "mse_results = {n: [] for n in neuron_range}\n",
        "melhor_mse = float(\"inf\")\n",
        "melhor_comb = (None, None)\n",
        "\n",
        "# Check if regression data is available\n",
        "if X_train_reg is not None and y_train_reg is not None and X_test_reg is not None and y_test_reg is not None:\n",
        "    for n in neuron_range:\n",
        "        print(f\"\\n>> Neurons: {n}\")\n",
        "        for ep in epoch_range:\n",
        "            # Use the regression model creation function and regression data\n",
        "            model = criar_modelo_reg(n, X_train_reg.shape[1], y_train_reg.shape[1])\n",
        "            model.fit(\n",
        "                X_train_reg, y_train_reg,\n",
        "                validation_split=0.2,\n",
        "                epochs=ep,\n",
        "                batch_size=16,\n",
        "                verbose=0\n",
        "            )\n",
        "            # Predict and calculate MSE using regression test data\n",
        "            y_pred = model.predict(X_test_reg)\n",
        "            mse = mean_squared_error(y_test_reg, y_pred)\n",
        "            mse_results[n].append(mse)\n",
        "\n",
        "            print(f\"   Epochs: {ep} → MSE: {mse:.4f}\")\n",
        "\n",
        "            if mse < melhor_mse:\n",
        "                melhor_mse = mse\n",
        "                melhor_comb = (n, ep)\n",
        "\n",
        "    # ===== Plot MSE vs Épocas =====\n",
        "    plt.figure(figsize=(10,6))\n",
        "    for n in neuron_range:\n",
        "        plt.plot(epoch_range, mse_results[n], marker='o', label=f'{n} neurons')\n",
        "\n",
        "    plt.title(\"Mean Squared Error (MSE) vs Epochs\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"MSE on the test set\")\n",
        "    plt.legend(title=\"Hidden layer\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Regression data (X_train_reg, y_train_reg, X_test_reg, y_test_reg) not found. Please run the previous cells to generate this data.\")\n",
        "    melhor_comb = (None, None) # Reset melhor_comb if data is missing / Redefinir melhor_comb se os dados estiverem faltando\n",
        "    melhor_mse = float(\"inf\") # Reset melhor_mse as well / Redefinir mrlhor_mse também"
      ],
      "metadata": {
        "id": "-jgPSv2Iy12N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Resultado melhor combinação / Best combination result =====\n",
        "melhor_n, melhor_ep = melhor_comb\n",
        "print(f\"\\nBest configuration: {melhor_n} neurons and {melhor_ep} epochs (MSE: {melhor_mse:.4f})\")"
      ],
      "metadata": {
        "id": "F3Cf7ULJ0MX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Treinar modelo final com melhor combinação / Train final model with best combination =====\n",
        "# Use the regression model creation function / Use a função de criação do modelo de regressão\n",
        "model_reg_final = criar_modelo_reg(melhor_n, X_train_reg.shape[1], y_train_reg.shape[1])\n",
        "history_reg_final = model_reg_final.fit(\n",
        "    X_train_reg, y_train_reg,\n",
        "    validation_split=0.2,\n",
        "    epochs=melhor_ep,\n",
        "    batch_size=16,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Avaliar modelo final com métricas de regressão / Evaluate final model with regression metrics\n",
        "y_pred_reg_final = model_reg_final.predict(X_test_reg)\n",
        "mse_final = mean_squared_error(y_test_reg, y_pred_reg_final)\n",
        "r2_final = r2_score(y_test_reg, y_pred_reg_final)\n",
        "\n",
        "\n",
        "print(f\"\\nTest Mean Squared Error: {mse_final:.4f}\")\n",
        "print(f\"Test R-squared: {r2_final:.4f}\")"
      ],
      "metadata": {
        "id": "H47c_OFJ0Tv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Mostrar e salvar pesos aprendidos / Show and save learned weights =====\n",
        "for i, layer in enumerate(model_reg_final.layers):\n",
        "    weights, biases = layer.get_weights()\n",
        "    layer_name = layer.name\n",
        "\n",
        "    print(f\"\\nLayer {i+1} - {layer_name}\")\n",
        "    print(\"Weights (W):\")\n",
        "    print(weights)\n",
        "    print(\"Biases (b):\")\n",
        "    print(biases)\n",
        "\n",
        "    # Salvar Pesos\n",
        "    weights_df = pd.DataFrame(weights)\n",
        "    weights_csv_filename = f\"weights_{layer_name}.csv\"\n",
        "    weights_excel_filename = f\"weights_{layer_name}.xlsx\"\n",
        "\n",
        "    weights_df.to_csv(weights_csv_filename, index=False)\n",
        "    weights_df.to_excel(weights_excel_filename, index=False)\n",
        "    print(f\"Layer weights {layer_name} saved in {weights_csv_filename} e {weights_excel_filename}\")\n",
        "\n",
        "    # Salvar Biases (verificar se a camada tem biases) / Save Biases (check if the layer has biases)\n",
        "    if len(biases) > 0:\n",
        "        biases_df = pd.DataFrame(biases, columns=['Bias'])\n",
        "        biases_csv_filename = f\"biases_{layer_name}.csv\"\n",
        "        biases_excel_filename = f\"biases_{layer_name}.xlsx\"\n",
        "\n",
        "        biases_df.to_csv(biases_csv_filename, index=False)\n",
        "        biases_df.to_excel(biases_excel_filename, index=False)\n",
        "        print(f\"Layer biases {layer_name} saved in {biases_csv_filename} and {biases_excel_filename}\")\n",
        "    else:\n",
        "        print(f\"Layer {layer_name} has no biases.\")"
      ],
      "metadata": {
        "id": "rfucup6z1AvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training history for regression / Traçando o histórico de treinamento para regressão\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history_reg_final.history['loss'], label='Train')\n",
        "plt.plot(history_reg_final.history['val_loss'], label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve - Regression')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uh9hyMzSHkIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressão / Regression"
      ],
      "metadata": {
        "id": "EqTFMzOY1L1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_column_names = ['feature1', 'feature2'] # Change to a list of your desired target columns / Alterar para uma lista de suas colunas de destino desejadas\n",
        "\n",
        "# Check if all target columns exist in the DataFrame / Verifica se todas as colunas de destino existem no DataFrame\n",
        "if all(col in df.columns for col in target_column_names):\n",
        "    y_reg = df[target_column_names].values\n",
        "    # Define features for regression, excluding the classification target and regression targets / Definir recursos para regressão, excluindo o alvo de classificação e os alvos de regressão\n",
        "    # Assuming the first column is the classification target / Supondo que a primeira coluna seja o alvo da classificação\n",
        "    feature_columns_reg = [col for col in df.columns if col not in [df.columns[0]] + target_column_names]\n",
        "    X_reg = df[feature_columns_reg].values\n",
        "\n",
        "else:\n",
        "    missing_cols = [col for col in target_column_names if col not in df.columns]\n",
        "    print(f\"Columns {missing_cols} not found in the DataFrame. Please check the column names.\")\n",
        "    y_reg = None # Set y_reg to None if columns are missing / Define y_reg como None se colunas estiverem faltando\n",
        "    X_reg = None # Also set X_reg to None if columns are missing / Define também X_reg como None se colunas estiverem faltando\n",
        "\n",
        "\n",
        "# Split data for regression only if target columns were found / Dividir dados para regressão somente se colunas de destino forem encontradas\n",
        "if y_reg is not None and X_reg is not None:\n",
        "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "        X_reg, y_reg, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Normalize data for regression / Normalizar dados para regressão\n",
        "    scaler_reg = StandardScaler()\n",
        "    X_train_reg = scaler_reg.fit_transform(X_train_reg)\n",
        "    # We don't normalize y_reg for this type of regression /Não normalizamos y_reg para este tipo de regressão\n",
        "    X_test_reg = scaler_reg.transform(X_test_reg)"
      ],
      "metadata": {
        "id": "dBOrxBqj1V-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define regression MLP model / Define MLP regression model\n",
        "neurons = 16 # número de neurônios na camada oculta / number of neurons in the hidden layer\n",
        "model_reg = Sequential([\n",
        "    Dense(neurons, input_dim=X_train_reg.shape[1], activation=\"relu\"),\n",
        "    Dense(len(target_column_names), activation='linear')  # Output layer for regression / Camada de saída para regressão\n",
        "])"
      ],
      "metadata": {
        "id": "U7PCnndzWQc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile regression model / Compilar modelo de regressão\n",
        "model_reg.compile(optimizer=\"adam\", loss=\"mse\") # Using Mean Squared Error for regression / Usando erro quadrático médio para regressão\n",
        "\n",
        "# Train regression model / treino do modelo de regressão\n",
        "history_reg = model_reg.fit(\n",
        "    X_train_reg, y_train_reg,\n",
        "    validation_split=0.2,\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate regression model / Avaliar modelo de regressão\n",
        "y_pred_reg = model_reg.predict(X_test_reg)\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ],
      "metadata": {
        "id": "e68w34j1VeWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca43f7f4"
      },
      "source": [
        "# ===== Gráficos de comparação: Real vs Predito (Regressão) / Comparison Charts: Actual vs Predicted (Regression) =====\n",
        "\n",
        "if y_test_reg is not None and y_pred_reg is not None:\n",
        "    num_targets = y_test_reg.shape[1]\n",
        "\n",
        "    plt.figure(figsize=(6 * num_targets, 5)) # Ajusta o tamanho da figura com base no número de alvos / Adjusts the size of the figure based on the number of targets\n",
        "\n",
        "    for i in range(num_targets):\n",
        "        plt.subplot(1, num_targets, i + 1)\n",
        "        plt.scatter(y_test_reg[:, i], y_pred_reg[:, i], alpha=0.5)\n",
        "        plt.xlabel(f\"Real - {target_column_names[i]}\")\n",
        "        plt.ylabel(f\"Predicted - {target_column_names[i]}\")\n",
        "        plt.title(f\"Actual vs Predicted Comparison for {target_column_names[i]}\")\n",
        "        plt.plot([y_test_reg[:, i].min(), y_test_reg[:, i].max()],\n",
        "                 [y_test_reg[:, i].min(), y_test_reg[:, i].max()],\n",
        "                 'k--', lw=2) # Adiciona linha de referência / Add reference line\n",
        "\n",
        "        # Calculate R-squared for the current target variable / Calcular R-quadrado para a variável alvo atual\n",
        "        r2_target = r2_score(y_test_reg[:, i], y_pred_reg[:, i])\n",
        "        plt.text(0.05, 0.95, f'R² = {r2_target:.2f}', transform=plt.gca().transAxes, fontsize=10,\n",
        "                 verticalalignment='top')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Test data (y_test_reg) or predictions (y_pred_reg) not found. Run the previous codes to generate this data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfico: Curva Real vs Predita (linha) para cada variável / Graph: Actual vs Predicted Curve (line) for each variable\n",
        "for i, target in enumerate(target_column_names):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(y_test_reg[:, i], label='Real', marker='o', linestyle='-')\n",
        "    plt.plot(y_pred_reg[:, i], label='Predicted', marker='x', linestyle='-')\n",
        "    plt.title(f\"Actual vs Predicted Curve - {target}\")\n",
        "    plt.xlabel(\"Sample Index\")\n",
        "    plt.ylabel(f\"Value of {target}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "odYQasjNPnn-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}